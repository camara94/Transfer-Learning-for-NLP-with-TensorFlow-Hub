Hi there.
Welcome to this.
Hands on.
Guided Project on Transfer Learning for Natural Language
processing with TensorFlow and TensorFlow Hub.
I hope you're having an amazing day.
I'm Snow Han, a data science and machine learning instructor
here at the Corsair, a project network.
And I'll be your instructor for today.
By the time you complete this project, you will be able to
use various pre trained natural language processing models
from TensorFlow Hub and you'll be able to perform transfer
learning to fine tune models on riel World text data.
And finally, you will visualize model performance metrics
with this tool from TensorFlow called Tensor Board.
But before we get started, you should know that there are a
few prerequisites that you need to meet for this project.
So in order to be successful, it is assumed that you are
familiar with Python as a programming language and you have
some prior experience with deep learning concepts and NLP
concepts.
And ideally, you have trained some simple models, simple deep
learning models with TensorFlow or its carries a P. I now let
me use the remainder of this task to set the context for you
in terms of what we will be covering in this project.
So in the real world, when you want to solve a problem with
some machine learning techniques, do you first need a
suitable model?
And now, every day, more powerful models are being published
in research papers.
Aaron blogged posts.
So let's say you come across one of these models and you want
to see how suitable this model is for your specific problem.
And the issues there that you might UH, encounter is that you
might have to do a lot of taking online to find the model
code and the repositories on Git Hub or somewhere else.
And if you're lucky, the researchers may have published the
pre trained model, and the model waits there as well.
And sometimes you may need to download a script and run it to
get access to the model.
I'm sure you can see the issues here right?
It can be hard to find the code and then get access to the
model. And a lot of the times you have genuine legitimate
questions about how do I use this model in my own work flow?
And how was the model trained and on what data was a trained.
And since the time I found out about this model, has the
model been updated?
And if so, am I using the right version?
So these are all the questions that might come up when you
want to use certain pre trained, um, models are new models
and architectures that you come across.
And this is where TensorFlow hub comes in handy.
So TF hub dot dev is the place for all your TensorFlow model
needs to easily find the latest ready to use models with
documentation, code snippets and much, much more.
So TensorFlow shows, uh, TensorFlow Hub is a rich
repositories of models which covers a wide range of machine
learning tasks for all your common ml needs.
For example, if you're doing computer vision or, you know,
working in, uh, image related tasks, TensorFlow hub has
models for, uh, image classification, object detection, image,
augmentation and so on.
But in this project, we are going to be focusing on text data.
So for text, T t f Hub has state of the art models like Bert
and, uh, Albert and Universal Sentence encoders and Elmo and
so on. So TensorFlow hub is this repositories off all these
pre trained models and many more pre trained text and
beddings that can support a wide range of natural language,
understanding tasks such as question answering text
classifications like we're going to do today and even
semantic analysis and many, many more.
But today we are going to focus our attention to pre trained,
uh, text and beddings and their associate ID models,
including the Universal Sentence and Coder natural language
modeling models and in L. M's and so on.
And these models can be used as ordinary caress layers and
can replace the traditional embedding layers that you would
use in your models.
So say you are using the sequential class to build a model,
and then, traditionally, you would have a nem bedding layer
to map your raw input.
Text data into either word embedding Zohra sentence of
beddings, and then you would have the rest of your network.
It could be some variant of R N N's, or it could just be, uh,
densely, fully connected layers followed by a classification
head. So the beauty of using TF hub is that you can just plug
in, play these pre trained models as unordinary care, a
slayer and some other amazing features, or at least I find
them amazing are that these modules include text pre
processing as part of their TensorFlow graph.
And that means that we will not have to do any P processing
of the text data ourselves.
This pretty amazing, right?
So we can simply pass our text to our model, which will
appropriately token eyes and pre process the text and
generate thes text and beddings and weekend later downstream.
Fine tune these M beddings if we choose to or past thes
embedding Xas is to a couple of dense layers downstream for
classifications.
And this way we can efficiently leverage huge amount of
compute dedicated by huge capital intensive and capital rich
organizations like Google and Opening I and many others who
who have dedicated a large amount of computing power to this
to learn these m beddings and representations by pre training
on massive amounts of data.
So they have done most of the work for us.
We just need to plug it into our workflow.
That said, we will not be covering the details of each
individual model, uh, in depth, and I ask you to spend some
time on that.
After this project, I will just briefly describe to you what
the model, what the models are and how are what data they
were pretty trained on.
And then we're just going to, uh, use those models as is
within our, uh, workflow.
So very simply gonna plug and play if you will.
These pre trained models to solve a real world problem and
the real world problem that we will be solving is to detect
toxic content online to improve, uh, the quality of online
discussions.
So, one of the most well known question answer sites, Quora
released a data set as part of a casual competition, and this
labeled data set contains questions asked on the quarrel
platform and the corresponding labels.
Uh, if the question was sincere or not.
Basically insincere questions are those that are considered
to be toxic, and they can include things like the question,
having a non neutral tone and being aggressive or disparaging
and inflammatory or based on false information and containing,
uh, sexually explicit content for shock value.
And the goal here is to predict whether a given question in
the data set is sincere or not.
And if it's sincere, uh, it has a class label of zero.
And if the question is insincere, the it has a label of one.
All right, so this is the problem that we're gonna be
tackling using transfer learning.
All right.
I hope this has given you a least a brief idea and overview
off what we're going to do in the remainder of this project.
Now, in the remainder of this project, let's actually start
writing code for it.
And to do that, we are going to be working within the Google
Co. Lab environment.
And if you haven't used it before, I'm glad I'm introducing
you to it because this is amazing environment that offers
free Jupiter notebooks with access to high performance GP use.
And in the next task, you're going to have to sign into
Google Collab from this link here.
And to do that, let's move on to the next task.
I'll walk you step by step through copying this template
notebook to your own account and how to set up TensorFlow
with GPU access in Google Collab.
See you in the next task.